
@book{marine_board_worker_2013,
	location = {Washington, D.C.},
	title = {Worker Health and Safety on Offshore Wind Farms - Special Report 310},
	isbn = {978-0-309-28496-7},
	url = {http://www.nap.edu/catalog/18327},
	publisher = {National Academies Press},
	editora = {{Marine Board} and {Transportation Research Board}},
	editoratype = {collaborator},
	urldate = {2024-06-16},
	date = {2013-04-10},
	doi = {10.17226/18327},
	keywords = {Industry and Labor--Policy, Reviews and Evaluations, Industry and Labor--Workforce and Labor Issues, safety},
}

@online{noauthor_windexchange_nodate,
	title = {{WINDExchange}: Offshore Wind Workforce Safety Standards \& Training Resource},
	url = {https://windexchange.energy.gov/offshore-workforce-safety-training},
	urldate = {2024-06-16},
	keywords = {safety},
	file = {WINDExchange\: Offshore Wind Workforce Safety Standards & Training Resource:/home/user/Zotero/storage/FCEGP6M7/offshore-workforce-safety-training.html:text/html},
}

@article{vidal_structural_2020,
	title = {Structural Health Monitoring for Jacket-Type Offshore Wind Turbines: Experimental Proof of Concept},
	volume = {20},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/7/1835},
	doi = {10.3390/s20071835},
	shorttitle = {Structural Health Monitoring for Jacket-Type Offshore Wind Turbines},
	abstract = {Structural health monitoring for offshore wind turbines is imperative. Offshore wind energy is progressively attained at greater water depths, beyond 30 m, where jacket foundations are presently the best solution to cope with the harsh environment (extreme sites with poor soil conditions). Structural integrity is of key importance in these underwater structures. In this work, a methodology for the diagnosis of structural damage in jacket-type foundations is stated. The method is based on the criterion that any damage or structural change produces variations in the vibrational response of the structure. Most studies in this area are, primarily, focused on the case of measurable input excitation and vibration response signals. Nevertheless, in this paper it is assumed that the only available excitation, the wind, is not measurable. Therefore, using vibration-response-only accelerometer information, a data-driven approach is developed following the next steps: (i) the wind is simulated as a Gaussian white noise and the accelerometer data are collected; (ii) the data are pre-processed using group-reshape and column-scaling; (iii) principal component analysis is used for both linear dimensionality reduction and feature extraction; finally, (iv) two different machine-learning algorithms, k nearest neighbor (k-{NN}) and quadratic-kernel support vector machine ({SVM}), are tested as classifiers. The overall accuracy is estimated by 5-fold cross-validation. The proposed approach is experimentally validated in a laboratory small-scale structure. The results manifest the reliability of the stated fault diagnosis method being the best performance given by the {SVM} classifier.},
	pages = {1835},
	number = {7},
	journaltitle = {Sensors},
	shortjournal = {Sensors},
	author = {Vidal, Yolanda and Aquino, Gabriela and Pozo, Francesc and Gutiérrez-Arias, José Eligio Moisés},
	urldate = {2024-09-24},
	date = {2020-03-26},
	langid = {english},
	file = {Full Text:/home/user/Zotero/storage/ZFSS34BI/Vidal et al. - 2020 - Structural Health Monitoring for Jacket-Type Offsh.pdf:application/pdf},
}

@inproceedings{leon_medina_online_2023,
	location = {Patras, Greece},
	title = {Online Structural Damage Classification Methodology For Offshore Wind Turbine Foundations Using Data Stream Analysis},
	isbn = {978-960-88104-6-4},
	url = {https://www.eccomasproceedia.org/conferences/thematic-conferences/smart-2023/9839},
	doi = {10.7712/150123.9839.445442},
	eventtitle = {10th {ECCOMAS} Thematic Conference on Smart Structures and Materials},
	pages = {866--872},
	booktitle = {10th {ECCOMAS} Thematic Conference on Smart Structures and Materials},
	publisher = {Dept. of Mechanical Engineering \& Aeronautics University of Patras},
	author = {Leon Medina, Jersson Xavier and Parés, Núria and Pozo, Francesc},
	urldate = {2024-10-28},
	date = {2023},
	langid = {english},
	file = {Full Text:/home/user/Zotero/storage/BWYPC55Z/Leon Medina et al. - 2023 - ONLINE STRUCTURAL DAMAGE CLASSIFICATION METHODOLOG.pdf:application/pdf},
}

@misc{grandini_metrics_2020,
	title = {Metrics for Multi-Class Classification: an Overview},
	url = {http://arxiv.org/abs/2008.05756},
	doi = {10.48550/arXiv.2008.05756},
	shorttitle = {Metrics for Multi-Class Classification},
	abstract = {Classification tasks in machine learning involving more than two classes are known by the name of "multi-class classification". Performance indicators are very useful when the aim is to evaluate and compare different classification models or machine learning techniques. Many metrics come in handy to test the ability of a multi-class classifier. Those metrics turn out to be useful at different stage of the development process, e.g. comparing the performance of two different models or analysing the behaviour of the same model by tuning different parameters. In this white paper we review a list of the most promising multi-class metrics, we highlight their advantages and disadvantages and show their possible usages during the development of a classification model.},
	number = {{arXiv}:2008.05756},
	publisher = {{arXiv}},
	author = {Grandini, Margherita and Bagli, Enrico and Visani, Giorgio},
	urldate = {2024-10-29},
	date = {2020-08-13},
	eprinttype = {arxiv},
	eprint = {2008.05756},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/home/user/Zotero/storage/ER79BCSU/Grandini et al. - 2020 - Metrics for Multi-Class Classification an Overvie.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/SSQ9EU3Z/2008.html:text/html},
}

@article{de_diego_general_2022,
	title = {General Performance Score for classification problems},
	volume = {52},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-021-03041-7},
	doi = {10.1007/s10489-021-03041-7},
	abstract = {Several performance metrics are currently available to evaluate the performance of Machine Learning ({ML}) models in classification problems. {ML} models are usually assessed using a single measure because it facilitates the comparison between several models. However, there is no silver bullet since each performance metric emphasizes a different aspect of the classification. Thus, the choice depends on the particular requirements and characteristics of the problem. An additional problem arises in multi-class classification problems, since most of the well-known metrics are only directly applicable to binary classification problems. In this paper, we propose the General Performance Score ({GPS}), a methodological approach to build performance metrics for binary and multi-class classification problems. The basic idea behind {GPS} is to combine a set of individual metrics, penalising low values in any of them. Thus, users can combine several performance metrics that are relevant in the particular problem based on their preferences obtaining a conservative combination. Different {GPS}-based performance metrics are compared with alternatives in classification problems using real and simulated datasets. The metrics built using the proposed method improve the stability and explainability of the usual performance metrics. Finally, the {GPS} brings benefits in both new research lines and practical usage, where performance metrics tailored for each particular problem are considered.},
	pages = {12049--12063},
	number = {10},
	journaltitle = {Applied Intelligence},
	shortjournal = {Appl Intell},
	author = {De Diego, Isaac Martín and Redondo, Ana R. and Fernández, Rubén R. and Navarro, Jorge and Moguerza, Javier M.},
	urldate = {2024-10-29},
	date = {2022-08-01},
	langid = {english},
	keywords = {Artificial Intelligence, Binary classification, Combination of information, Explainability, Multi-class classification, Performance metrics},
	file = {Full Text PDF:/home/user/Zotero/storage/UMX5NGVG/De Diego et al. - 2022 - General Performance Score for classification probl.pdf:application/pdf},
}

@article{chicco_advantages_2020,
	title = {The advantages of the Matthews correlation coefficient ({MCC}) over F1 score and accuracy in binary classification evaluation},
	volume = {21},
	issn = {1471-2164},
	url = {https://doi.org/10.1186/s12864-019-6413-7},
	doi = {10.1186/s12864-019-6413-7},
	abstract = {To evaluate binary classifications and their confusion matrices, scientific researchers can employ several statistical rates, accordingly to the goal of the experiment they are investigating. Despite being a crucial issue in machine learning, no widespread consensus has been reached on a unified elective chosen measure yet. Accuracy and F1 score computed on confusion matrices have been (and still are) among the most popular adopted metrics in binary classification tasks. However, these statistical measures can dangerously show overoptimistic inflated results, especially on imbalanced datasets.},
	pages = {6},
	number = {1},
	journaltitle = {{BMC} Genomics},
	shortjournal = {{BMC} Genomics},
	author = {Chicco, Davide and Jurman, Giuseppe},
	urldate = {2024-10-29},
	date = {2020-01-02},
	keywords = {Binary classification, Accuracy, Biostatistics, Confusion matrices, Dataset imbalance, F1 score, Genomics, Machine learning, Matthews correlation coefficient},
	file = {Full Text PDF:/home/user/Zotero/storage/PCB9C5NA/Chicco and Jurman - 2020 - The advantages of the Matthews correlation coeffic.pdf:application/pdf;Snapshot:/home/user/Zotero/storage/J4WENSBK/s12864-019-6413-7.html:text/html},
}

@article{tharwat_classification_2021,
	title = {Classification assessment methods},
	volume = {17},
	issn = {2634-1964, 2210-8327},
	url = {https://doi.org/10.1016/j.aci.2018.08.003},
	doi = {10.1016/j.aci.2018.08.003},
	abstract = {Classification techniques have been applied to many applications in various fields of sciences. There are several ways of evaluating classification algorithms. The analysis of such metrics and its significance must be interpreted correctly for evaluating different learning algorithms. Most of these measures are scalar metrics and some of them are graphical methods. This paper introduces a detailed overview of the classification assessment measures with the aim of providing the basics of these measures and to show how it works to serve as a comprehensive source for researchers who are interested in this field. This overview starts by highlighting the definition of the confusion matrix in binary and multi-class classification problems. Many classification measures are also explained in details, and the influence of balanced and imbalanced data on each metric is presented. An illustrative example is introduced to show (1) how to calculate these measures in binary and multi-class classification problems, and (2) the robustness of some measures against balanced and imbalanced data. Moreover, some graphical measures such as Receiver operating characteristics ({ROC}), Precision-Recall, and Detection error trade-off ({DET}) curves are presented with details. Additionally, in a step-by-step approach, different numerical examples are demonstrated to explain the preprocessing steps of plotting {ROC}, {PR}, and {DET} curves.},
	pages = {168--192},
	number = {1},
	journaltitle = {Applied Computing and Informatics},
	author = {Tharwat, Alaa},
	urldate = {2024-10-29},
	date = {2021-01-01},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Assessment methods, Classification, Confusion matrix, Precision-Recall ({PR}) curve, Receiver operating characteristics ({ROC})},
	file = {Full Text PDF:/home/user/Zotero/storage/YRUJTT4Q/Tharwat - 2021 - Classification assessment methods.pdf:application/pdf},
}

@collection{sammut_encyclopedia_2017,
	location = {New York, {NY}},
	edition = {expanded and updated second edition},
	title = {Encyclopedia of machine learning and data mining},
	isbn = {978-1-4899-7685-7 978-1-4899-7687-1},
	series = {{SpringerLink} Bücher},
	pagetotal = {263},
	publisher = {Springer},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	date = {2017},
	doi = {10.1007/978-1-4899-7687-1},
	file = {Sammut and Webb - 2017 - Encyclopedia of machine learning and data mining.pdf:/home/user/Zotero/storage/RW37V8AK/Sammut and Webb - 2017 - Encyclopedia of machine learning and data mining.pdf:application/pdf},
}

@book{daniel_zwillinger_standard_nodate,
	title = {Standard Mathematical Tables And Formulae 31st Edition Zwillinger},
	url = {http://archive.org/details/StandardMathematicalTablesAndFormulae31stEditionZwillinger},
	abstract = {In this book many formulae and tables are given which can helpfull in preparation of {IIT} {JEE} and other competitive engineering/science exams.},
	author = {{Daniel Zwillinger}},
	urldate = {2024-11-05},
	keywords = {Mathematics Formulae},
	file = {Daniel Zwillinger - Standard Mathematical Tables And Formulae 31st Edi.pdf:/home/user/Zotero/storage/8IZDGEGA/Daniel Zwillinger - Standard Mathematical Tables And Formulae 31st Edi.pdf:application/pdf},
}

@book{abramowitz_handbook_1964,
	title = {Handbook of mathematical functions with formulas, graphs, and mathematical tables},
	rights = {Publications of the National Institute of Standards and Technology ({NIST}) and National Bureau of Standards ({NBS}) are published by the U.S. Government. They are in the public domain and not subject to copyright in the United States. However, please pay special attention to the individual works to make sure there are no copyright restrictions indicated. Individual works may require securing other permissions from the original copyright holder.},
	url = {http://archive.org/details/handbookofmathem1964abra},
	abstract = {"Trivia question: What is the most cited work in the mathematical literature? With an estimated 40,000 citations, the Handbook of Mathematical Functions may well be it. Edited by Milton Abramowitz and Irene Stegun and released by the National Bureau of Standards in 1964, the Handbook was the result of a ten-year project to  compile essential information on the special functions of applied mathematics (e.g., Bessel functions, hypergeometric functions, and orthogonal polynomials) for use in applications. The Handbook remains highly relevant today in spite of its age... The number of citations yearly has been steadily increasing since 1964." - Ronald Boisvert et al, "Handbook for the Digital Age," Notices of the {AMS}, August 2011. http://www.ams.org/notices/201107/rtx110700905p.pdf, accessed 9/12/2012},
	pagetotal = {1084},
	publisher = {U.S. Department of Commerce, National Bureau of Standards},
	author = {Abramowitz, Milton (editor) and Stegun, Irena A. (editor)},
	editora = {{NIST Research Library}},
	editoratype = {collaborator},
	urldate = {2024-11-05},
	date = {1964},
	keywords = {Functions},
	file = {Abramowitz and Stegun - 1964 - Handbook of mathematical functions with formulas, .pdf:/home/user/Zotero/storage/LIGX54YM/Abramowitz and Stegun - 1964 - Handbook of mathematical functions with formulas, .pdf:application/pdf},
}

@inproceedings{akosa_predictive_2017,
	title = {Predictive Accuracy : A Misleading Performance Measure for Highly Imbalanced Data},
	url = {https://www.semanticscholar.org/paper/Predictive-Accuracy-%3A-A-Misleading-Performance-for-Akosa/8eff162ba887b6ed3091d5b6aa1a89e23342cb5c},
	shorttitle = {Predictive Accuracy},
	abstract = {The most commonly reported model evaluation metric is the accuracy. This metric can be misleading when the data are imbalanced. In such cases, other evaluation metrics should be considered in addition to the accuracy. This study reviews alternative evaluation metrics for assessing the effectiveness of a model in highly imbalanced data. We used credit card clients in Taiwan as a case study. The data set contains 30,000 instances (22.12\% risky and 77.88\% non-risky) assessing the likeliness of a customer defaulting on a payment. Three different techniques were used during the model building process. The first technique involved down-sampling the majority class in the training subset. The second used the original imbalanced data whereas prior probabilities were set to account for oversampling in the third technique. The same sets of predictive models were then built for each technique after which the evaluation metrics were computed. The results suggest that model evaluation metrics might reveal more about distribution of classes than they do about the actual performance of models when the data are imbalanced. Moreover, some of the predictive models were identified to be very sensitive to imbalance. The final decision in model selection should consider a combination of different measures instead of relying on one measure. To minimize imbalance-biased estimates of performance, we recommend reporting both the obtained metric values and the degree of imbalance in the data. {INTRODUCTION} One of the biggest challenges in data mining is dealing with highly imbalanced data sets. We encounter imbalanced data in several real world applications including, credit card fraud detection, churn prediction, customer retention, and medical diagnostics among many others. An imbalance occurs when one or more classes (minority class) have very low proportions in the data as compared to the other classes (majority class). Mostly in these situations, the main interest is in correctly classifying the minority class. However, the most commonly used classification algorithms do not work well for such problems. This is because the classifiers tend to be biased towards the majority class and hence perform poorly on the minority class. Several different techniques have been proposed to solve the problems associated with learning from class-imbalanced data. One of such techniques is based on cost sensitive learning. Here, a high cost is assigned to misclassification of the minority class while trying to minimize the overall cost. For instance, an analyst might have reasons to believe that misclassifying the minority class (false negatives) is X times costlier than misclassifying the majority class (false positives). The addition of specific costs during model training will bias the model towards the minority class thereby affecting the model parameters. This therefore has a potential of improving upon the performance of the model. Another technique is to use a sampling technique during model training: either down-sample the majority class or over-sample the minority class. Down-sampling is a technique utilized to reduce the number of samples in the majority class to obtain roughly equal data points across the classes. Up-sampling on the other hand simulates data points to enhance balance across the classes. Even though the use of sampling techniques can introduce bias into the model results, these techniques can still be effective during the tuning of model parameters. Despite the improvements of the above techniques on model performance during parameter tuning, we note that the best performing model is not chosen based on the performance measure of the training subset but on the testing subset. The distribution of the testing data may differ from that of the training data, and the true misclassification costs may be unknown at learning time. In addition, the testing data needs to be consistent and reflect the state of nature of the real data in order to produce honest estimates of future events. Consequently, sampling techniques cannot be applied to the testing data to fairly balance the class distribution. In such situations, it is the duty of the researcher or practitioner to determine an appropriate performance measure to use when choosing between different classifiers.},
	author = {Akosa, J.},
	urldate = {2024-11-09},
	date = {2017},
	file = {Full Text PDF:/home/user/Zotero/storage/LU72SSSQ/Akosa - 2017 - Predictive Accuracy  A Misleading Performance Mea.pdf:application/pdf},
}
